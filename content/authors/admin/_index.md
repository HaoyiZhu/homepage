---
title: Haoyi Zhu æœ±çš“æ€¡
role: Ph.D student in Computer Science
avatar_filename: avatar.jpg
bio: My research interests include embodied AI, computer vision and robot learning.
interests:
  - Embodied AI
  - Computer Vision
  - Robot Learning
social:
  - icon: envelope
    icon_pack: fas
    link: /#contact
  - icon: twitter
    icon_pack: fab
    link: https://twitter.com/HaoyiZhu
    label: Follow me on Twitter
    display:
      header: true
  - icon: graduation-cap
    icon_pack: fas
    link: https://scholar.google.com/citations?user=pD1NOyUAAAAJ&hl
    label: Visit my Google Scholar
    display:
      header: true
  - icon: github
    icon_pack: fab
    link: https://github.com/HaoyiZhu
    label: Follow me or star on Github
    display:
      header: true
  - icon: orcid
    icon_pack: fab
    link: https://orcid.org/0000-0003-1153-5230
organizations:
  - name: Shanghai Artificial Intelligence Laboratory
    url: https://www.shlab.org.cn/
  - name: University of Science and Technology of China
    url: https://www.ustc.edu.cn/
education:
  courses:
    - course: B.S. in Artificial Intelligence Honor Class
      institution: Shanghai Jiao Tong University
      year: 2019 - 2023
    - course: Ph.D. in Computer Science
      institution: Shanghai AI Lab & USTC
      year: 2023 - Present
superuser: true
last_name: Zhu
highlight_name: true
first_name: Haoyi
email: hyizhu1108@gmail.com
---
I am a second-year Ph.D. student in Computer Science jointly at Shanghai Artificial Intelligence Laboratory and University of Science and Technology of China advised by Prof. [Xiaogang Wang](http://www.ee.cuhk.edu.hk/~xgwang/) and Prof. [Wanli Ouyang](https://wlouyang.github.io/). I work closely with Prof. [Tong He](https://tonghe90.github.io/). I earned my B.S. degree in Artificial Intelligence Honor Class at Shanghai Jiao Tong University, advised by Prof. [Cewu Lu](https://mvig.sjtu.edu.cn/). I also have had the privilege of working with Dr. [Hao-Shu Fang](https://fang-haoshu.github.io/) and Dr. [Jim Fan](https://jimfan.me/). My research interests focus on embodied AI and robot manipulation. Feel free to follow me on [<i class="fa-brands fa-twitter"></i>](https://twitter.com/HaoyiZhu) and [<i class="fa-brands fa-github"></i>](https://github.com/HaoyiZhu) for latest research announcements and updates!

In my personal life, I am passionate (but amateur) about football, music, literature, philosophy, [traditional Chinese painting](#gallery), and [modern Chinese poems](#poems)!

*The philosophers have only interpreted the world, in various ways. The point, however, is to change it.*

### âœ¨ **News** âœ¨

- **Sep. 2024**: [PointCloudMatters](https://arxiv.org/abs/2402.02500) is accepted by NeurIPS D&B 2024! We prove that explicit representation like point cloud can significantly enhance the performance and generalization ability of robot learning policies. [Codes <i class="fa-brands fa-github"></i>](https://github.com/HaoyiZhu/PointCloudMatters) are open-sourced!
- **Feb. 2024**: [UniPAD](https://arxiv.org/abs/2310.08370) is accepted by CVPR 2024! Check out our [code on  <i class="fa-brands fa-github"></i>](https://github.com/Nightmare-n/UniPAD)!
- **Oct. 2023**: [PonderV2](https://arxiv.org/abs/2310.08586) and [UniPAD](https://arxiv.org/abs/2310.08370) has been announced! [PonderV2](https://arxiv.org/abs/2310.08586) is a universal pre-training paradigm for 3D vision, paving the way for 3D foundation model. It achieves SOTA on 11 indoor and outdoor benchmarks. Check out our [paper](https://arxiv.org/abs/2310.08586) and [code](https://github.com/OpenGVLab/PonderV2)!
- **Jul. 2023**: [RH20T](https://rh20t.github.io/) has been announced! [RH20T](https://rh20t.github.io/) is a large-scale open-source robotic dataset for learning diverse skills in one-shot, comprising over **110,000** contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, **all collected in the real world**. Please check out our [website](https://rh20t.github.io/) for latest updates!
- **Nov. 2022**: [MineDojo](https://minedojo.org/) has won ðŸŽ‰ *Outstanding Paper Award* ðŸŽ‰ at NeurIPS [announcement](https://neurips.cc/virtual/2022/awards_detail)!
- **Nov. 2022**: [AlphaPose paper](http://arxiv.org/abs/2211.03375) is accepted by **TPAMI**! [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose) is an accurate multi-person pose estimator, which has received more than **6.5K** stars on Github. Check out [the paper](https://arxiv.org/pdf/2211.03375.pdf) for more details and feel free to [star on  <i class="fa-brands fa-github"></i>](https://github.com/MVIG-SJTU/AlphaPose)!
- **Oct. 2022**: [X-NeRF](https://arxiv.org/abs/2210.05135) is accepted by **WACV 2023**! Checkout our [code on  <i class="fa-brands fa-github"></i>](https://github.com/HaoyiZhu/XNeRF)!
- **Jun. 2022**: [MineDojo](https://minedojo.org/) has been [announced](https://twitter.com/DrJimFan/status/1540381991052247041)! [MineDojo](https://minedojo.org/) is a new framework for building generally capable agents with internet-scale knowledge in Minecraft. [Paper](https://arxiv.org/abs/2206.08853), [code](https://github.com/MineDojo/MineDojo), and [databases](https://minedojo.org/knowledge_base.html) are all open access. Check it out today!
