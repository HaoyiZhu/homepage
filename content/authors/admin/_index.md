---
title: Haoyi Zhu æœ±çš“æ€¡
role: Ph.D student in Computer Science
avatar_filename: avatar.jpg
bio: My research interests include World Model, Embodied AI and Spatial Intelligence.
interests:
  - World Model
  - Embodied AI
  - Spatial Intelligence
social:
  - icon: envelope
    icon_pack: fas
    link: /#contact
  - icon: twitter
    icon_pack: fab
    link: https://twitter.com/HaoyiZhu
    label: Follow me on Twitter
    display:
      header: true
  - icon: graduation-cap
    icon_pack: fas
    link: https://scholar.google.com/citations?user=pD1NOyUAAAAJ&hl
    label: Visit my Google Scholar
    display:
      header: true
  - icon: github
    icon_pack: fab
    link: https://github.com/HaoyiZhu
    label: Follow me or star on Github
    display:
      header: true
  - icon: orcid
    icon_pack: fab
    link: https://orcid.org/0000-0003-1153-5230
organizations:
  - name: USTC
    url: https://en.ustc.edu.cn/
  # - name: USTC
  #   url: https://www.ustc.edu.cn/
  # - name: [Curricilum Vitae]
  #   url: https://www.haoyizhu.site/uploads/CV.pdf

education:
  courses:
    - course: B.S. in Artificial Intelligence Honor Class
      institution: Shanghai Jiao Tong University
      year: 2019 - 2023
    - course: Ph.D. in Computer Science
      institution: University of Science and Technology of China
      year: 2023 - Present
superuser: true
last_name: Zhu
highlight_name: true
first_name: Haoyi
email: hyizhu1108@gmail.com
---
I am a third-year Ph.D. student in Computer Science at University of Science and Technology of China (USTC) advised by Prof. [Tong He](https://tonghe90.github.io/), Prof. [Wanli Ouyang](https://wlouyang.github.io/) and Prof. [Xiaogang Wang](http://www.ee.cuhk.edu.hk/~xgwang/). I earned my B.S. degree in Artificial Intelligence Honor Class at Shanghai Jiao Tong University (SJTU), advised by Prof. [Cewu Lu](https://mvig.sjtu.edu.cn/). I also have had the privilege of working with Dr. [Hao-Shu Fang](https://fang-haoshu.github.io/) and Dr. [Jim Fan](https://jimfan.me/). 

*Research for fun and truth.* My current research interests focus on World Model, Embodied AI and Spatial Intelligence. The ultimate goal of my life is to *discover myself, find the truth, and change the world!* Feel free to follow me on [<i class="fa-brands fa-twitter"></i>](https://twitter.com/HaoyiZhu) and [<i class="fa-brands fa-github"></i>](https://github.com/HaoyiZhu) for latest research announcements and updates!

In my personal life, I am passionate (but amateur) about football, music, literature, philosophy, [traditional Chinese painting](#gallery), and [modern Chinese poems](#poems)!

*"The philosophers have only interpreted the world, in various ways. The point, however, is to change it."*

### âœ¨ **News** âœ¨

- **Jan. 2025**: [$\pi^3$](https://yyfz.github.io/pi3/), [WinT3R](https://lizizun.github.io/WinT3R.github.io/), and [OmniWorld](https://yangzhou24.github.io/OmniWorld/) have been accepted by ICLR 2026!
- **Oct. 2025**: ðŸŽ‰ðŸŽ‰ðŸŽ‰ I have won the **National Scholarship** and the **First Price Academic Scholarship** at USTC! Thanks, USTC!
- **Oct. 2025**: [Aether](https://aether-world.github.io/) has won ðŸŽ‰ *Outstanding Paper Award* ðŸŽ‰ and has presented an *Oral Presentation* at ICCV 2025 RIWM workshop!
- **Sep. 2025**: [OmniWorld](https://yangzhou24.github.io/OmniWorld/) has been released! It is a multi-domain and multi-modal dataset for 4D world modeling. Check it out today!
- **Sep. 2025**: [WinT3R](https://lizizun.github.io/WinT3R.github.io/) has been announced! It is a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Check it out!
- **Jul. 2025**: [$\pi^3$](https://yyfz.github.io/pi3/) has been [announced](https://x.com/yyfz321021/status/1947135469294371225)! [$\pi^3$](https://yyfz.github.io/pi3/) is a novel feed-forward neural network that revolutionizes visual geometry reconstruction by eliminating the need for a fixed reference view. [Paper](https://arxiv.org/abs/2507.13347), [code](https://github.com/yyfz/Pi3), and [demo](https://huggingface.co/spaces/yyfz233/Pi3) are all open access. Check it out today!
- **Jun. 2025**: [Aether](https://aether-world.github.io/) and [VQ-VLA](https://xiaoxiao0406.github.io/vqvla.github.io/) are accepted by ICCV 2025!
- **Jun. 2025**: [DeepVerse](https://sotamak1r.github.io/deepverse/), an auto-regressive 4D world model, has been released!
- **Feb. 2025**: [SPA](https://haoyizhu.github.io/spa/) has been accepted by **ICLR 2025** and [Tra-MoE](https://arxiv.org/abs/2411.14519) has been accepted by **CVPR 2025**!
- **Oct. 2024**: [SPA](https://haoyizhu.github.io/spa/) has been [announced](https://x.com/HaoyiZhu/status/1844675411760013471)! [SPA](https://haoyizhu.github.io/spa/) is a novel representation learning framework that emphasizes the importance of 3D spatial awareness in embodied AI. [Paper](https://arxiv.org/abs/2410.08208), [code](https://github.com/HaoyiZhu/SPA), and [pre-trained models](https://huggingface.co/HaoyiZhu/SPA) are all open-sourced! Check it out!
- **Sep. 2024**: [PointCloudMatters](https://arxiv.org/abs/2402.02500) is accepted by NeurIPS D&B 2024! We prove that explicit representation like point cloud can significantly enhance the performance and generalization ability of robot learning policies. [Codes <i class="fa-brands fa-github"></i>](https://github.com/HaoyiZhu/PointCloudMatters) are open-sourced!
- **Oct. 2023**: [PonderV2](https://arxiv.org/abs/2310.08586) and [UniPAD](https://arxiv.org/abs/2310.08370) has been announced! [PonderV2](https://arxiv.org/abs/2310.08586) is a universal pre-training paradigm for 3D vision, paving the way for 3D foundation model. <!-- It achieves SOTA on 11 indoor and outdoor benchmarks. Check out our [paper](https://arxiv.org/abs/2310.08586) and [code](https://github.com/OpenGVLab/PonderV2)! -->
- **Jul. 2023**: [RH20T](https://rh20t.github.io/) has been announced! [RH20T](https://rh20t.github.io/) is a large-scale, open-source, real-world robotic dataset. <!-- , comprising over **110,000** contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, **all collected in the real world**. Please check out our [website](https://rh20t.github.io/) for latest updates! -->
- **Nov. 2022**: [MineDojo](https://minedojo.org/) has won ðŸŽ‰ *Outstanding Paper Award* ðŸŽ‰ at NeurIPS [announcement](https://neurips.cc/virtual/2022/awards_detail)!
- **Nov. 2022**: [AlphaPose paper](http://arxiv.org/abs/2211.03375) is accepted by **TPAMI**! [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose) is an accurate multi-person pose estimator, which has received more than **8.3K** stars on Github. <!-- Check out [the paper](https://arxiv.org/pdf/2211.03375.pdf) for more details and feel free to [star on  <i class="fa-brands fa-github"></i>](https://github.com/MVIG-SJTU/AlphaPose)! -->
- **Jun. 2022**: [MineDojo](https://minedojo.org/) has been [announced](https://twitter.com/DrJimFan/status/1540381991052247041)! [MineDojo](https://minedojo.org/) is a new framework for building generally capable agents with internet-scale knowledge in Minecraft. <!-- [Paper](https://arxiv.org/abs/2206.08853), [code](https://github.com/MineDojo/MineDojo), and [databases](https://minedojo.org/knowledge_base.html) are all open access. Check it out today! -->
